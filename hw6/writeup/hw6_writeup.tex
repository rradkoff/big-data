\documentclass[11pt, fleqn]{article}

\input{header.tex}

\setlength{\parskip}{12pt} % Sets a blank line in between paragraphs
\setlength\parindent{0pt} % Sets the indent for each paragraph to zero

\begin{document}

\title{Big Data: Homework 6}
\author{Will Clark \& Matthew DeLio \\ 41201-01}
\date{\today}
\maketitle

\section{K-Means Clustering} \label{sec:kmeans}

In this section, we cluster the 109th congress phrase-counts to see what information can be obtained.  To accomplish this, we employ the K-Means clustering algorithm.  To choose the ``optimal'' cluster size, we employed varied the cluster-size from 2 to 35 and then used BIC to select the K with the minimum BIC, which should, hopefully good OOS clustering abilities.  \Vref{fig:kmeans_ic_plot} shows a plot of the information criteria while varying K; while the BIC finds a minima at 15, the AICc never reaches a minima.

As discussed in class, these information criteria are of lower-quality than those used for regression.  A quick glance at \vref{tab:k_means_summary} backs up this theory as there are several clusters with just a single representative (leading us to believe that the cluster is overfitting), and no clear divide between the democrats, republicans, and independents.  A quick glance at the top-3 phrases used in each cluster is shown in \vref{tab:g_words}, shows that cluster phrases can be rather random.  Additionally, the bulk of the representatives are contained in a single cluster.  However, when the algorithm does have a clear separation between democrat and republican representatives, there is a clear divide.  Combining information from the cluster summary (\cref{tab:k_means_summary}) and the top-3 phrases table (\cref{tab:k_means_3}) phrases such as ``program.help'' vs ``illegal.immigration'' for Democrats and Republicans respectively indicate that the algorithm can cluster based on some partisan phrases.

As a final note, for an experiment to see if K-means would do a better job clustering the two parties, we ran k-means with a cluster-size of 2 \& 3.  In both instances (see \vref{tab:k_means_3} for K=3), the bulk of the representatives were clustered together as when K=15.  This begs the question: are the two parties really all that different after all?

\begin{figure}[!htb]
  \centering
  \includegraphics[scale=.5]{kmeans_ic_plot.eps}
  \caption{Informaton Criteria for Varying K-means}
  \label{fig:kmeans_ic_plot}
\end{figure}

\input{g_words.tex}

\section{Topic Model}

A topic model was constructed using \texttt{topics} command in R.  This multinomial model clusters the phrases used by the representatives into topics and is supposed to do a much better job than the K-means cluster from earlier.  The top-3 phrases for each topic (sorted by largest $P(phrase_i|topic_k)$) are found in \vref{tab:tpcs3}.  These phrases do indicate that this method does a really good job of finding related phrases.  There are few clusters that should probably be merged or otherwise modified (namely the 9th and 10th topics).  However, the vast majority of topics contain phrases that are very much inter-related; they are, in fact topics.  For example, the first topic appears to be about immigration, the second about civil rights, etc.

\input{tpcs3.tex}

\section{Connecting Unsupervised Clusters to Partisanship}
\subsection{Party Membership by K-means Cluster}
See \vref{tab:k_means_summary} for the party membership by K-means cluster.  As mentioned in the \cref{sec:kmeans}, the K-means clustering does not yield a ton of useful information about a representatives affiliation.  There are a handful of senators and congressman that are clearly grouped into their party, however, the vast majority end up in a single cluster.  This is likely because they argue against each other and therefore use each others' words in their speeches.  However, if taken at face-value, the cluster with almost an equal amount of republican and democratic members would likely denote phrases that are considered non-partisan.  These ``non-partisan'' phrases are found in \vref{tab:k_means_nonpart}, and do actually tend to indicate non-partisan phrases.

\input{k_means_summary.tex}
\input{k_means_nonpart.tex}

\subsection{Non-Partisan Topics}

Here we plot the topic-frequencies based on party affiliation.  \Vref{fig:topic} clearly shows that some topics are much more partisan than others.  For the topics with large deviation between the two parties, we can (using \vref{tab:tpcs3}) see that some topics, such as illegal immigration and civil rights, tend to align with our intuition on which phrases would be used more often by party members.  For the most part, these differences in frequencies exist and are quite noticeable.  However, one of the topics (\#7) appears to be non-partisan.  \Vref{tab:non_part_topics} shows the top 10 phrases from that topic and intuition confirms that many of these phrases are fairly non-partisan.

\begin{figure}[!htb]
  \centering
  \includegraphics[scale=.5]{topic.eps}
  \caption{Partisan Topic Frequencies}
  \label{fig:topic}
\end{figure}

\input{non_part_topics.tex}

\subsection{Party Membership/Share by Topic}

In this section, we use the results from the topic clustering to predict party membership.  We employ a gamma lasso with and without cross-validation to regress political affiliation onto the topic $\omega$'s chosen for each representative.  In general $\omega$'s give the percentage of a document (in this case a collection of speeches by a representative) that belongs to a given topic.  The hope is that the topics that are more likely to be discussed by, say a democrat, would help predict their party affiliation.  The gamma lasso and the cross-validated gamma lasso both include all 13 topics in the model, but completely disagree on the values of the information criteria (likely due to the small number of degrees of freedom).  \Vref{tab:topic_rep} shows that all three of the information criteria choose all 13 covariates and provide an in-sample $R^2$ of 56\%.  The OOS $R^2$ for the model chosen by cv.gamlr (using the CV.min rule) is a bit lower at $R^2=52\%$.

\input{topic_rep.tex}

Here we regress the ``republican share'' onto our topics.  While very similar to regressing the party membership above, this measure allows us to determine how much a the use of certain topics will predict how large the republican share is in one's district.  Note, that this might not predict who is a democrat vs a republican; in fact, we would expect that in contested districts/states, the republican share to be quite large even if the sitting representative is a democrat.  As before the gamma lasso with/without cross-validation is performed; the results are shown in \vref{tab:topic_repshare}.  Again we see that the in-sample $R^2$ each of the ICs is 37\%.  Using cross-validation and the CV.min selection rule, this $R^2$ drops to 35\%; just a touch lower than the in-sample ones.

\input{topic_repshare.tex}

Finally, we regress the party affiliation and republican share (separately) onto phrase frequency (measured in \%).  From \vref{tab:repx,tab:repsharex}, we see that, in both cases, AICc and CV.Min disagree quite a bit on the number of covariates to include.  Also, in-sample predictive power for the republican share (using AICc) is much higher than the out-of-sample one predicted/measured by the cross-validated gamma lasso.  Similarly, the $\lambda$ and $R^2$ found using AICc and CV.Min for the party affiliation also disagree widely.  This tends to indicate the phrase percentage, is not a great predictor of either of these. 

\input{repx.tex}
\input{repsharex.tex}

\Vref{tab:repx_coef_p,tab:repx_coef_n} show both the positive and negative the phrase percentage coefficients that predict the odds of a representative being a republican.  The signals selected are actually, intuitively quite good; however, there are not too many signals actually selected in the end (only 4 signals that increase the odds of a republican and 32 that decrease).  The model is perhaps underfit by the AICc, but, ultimately, by not distilling the phrases into a more usable form, the signals, by themselves are just not that strong.  In this sense, the distillation of the phrases into topics gives us a powerful tool to analyze data.

\input{repx_coef_p.tex}
\input{repx_coef_n.tex}


\clearpage
\section{Appendix}

\input{k_means_3.tex}

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{tpcs_rep.eps}
    \caption{Regressing Party Affiliation}
    \label{fig:tpcs_rep}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{tpcs_rep_share.eps}
    \caption{Regressing District Republican Share}
    \label{fig:tpcs_repshare}
  \end{subfigure}
  \caption{Topics as Predictors}
\end{figure}

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{reg_phrase_pcnt.eps}
    \caption{Regressing Party Affiliation}
    \label{fig:pcnt_rep}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth]{reg_phrase_pcnt_repshare.eps}
    \caption{Regressing District Republican Share}
    \label{fig:pcnt_rep_share}
  \end{subfigure}
  \caption{Phrase \% as Predictors}
\end{figure}


% \input{.tex}

% \begin{figure}
%   \centering
%   \begin{subfigure}[b]{0.49\textwidth}
%     \includegraphics[width=\textwidth]{.eps}
%     \caption{}
%     \label{fig:}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}[b]{0.49\textwidth}
%     \includegraphics[width=\textwidth]{.eps}
%     \caption{}
%     \label{fig:}
%   \end{subfigure}
%   \caption{}
% \end{figure}

% \begin{figure}[!htb]
%   \centering
%   \includegraphics[scale=.5]{.eps}
%   \caption{}
%   \label{fig:}
% \end{figure}

\end{document}